# -*- coding: utf-8 -*-
"""
Created on Tue Mar  8 11:42:14 2022

@author: yuwei
"""
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.autograd import Variable
import numpy as np
from torchvision import models

import numpy as np
import matplotlib.pyplot as plt
from sklearn.utils import class_weight

import torchvision.models as models
import torch.nn as nn

from collections import OrderedDict

torch.manual_seed(1)    # reproducible

# input image dir
data_dir = '../datasets/cnn_data/## perpare_ok_hand_more_suate_fall/'

# output weights dir
save_dir_weights = './cnn_weights/vgg_finish_model.pkl'
save_dir_plot = './cnn_training_plot/'

# =============================================================================
# Q：為什麼要使用class_weights?
# A：這是一種緩解資料不平衡的方法，因為兩類的影像數量差了近3倍，為了避免模型只會輸出類別較多的那類，雖然有可能指標很漂亮，但卻是無用的模型。
#    模型加入懲罰機制，當樣本數量越低則權重越高，意思是驅使模型用更多力氣去學習那一類，而權重不高的那類就不那麼在意。
# =============================================================================

# Hyper Parameters
input_size = 224
n_epochs = 300
batch_size = 32
class_names = ['F', 'N']
learning_rate = 0.0001
patience = 10
class_weights=class_weight.compute_class_weight(class_weight = 'balanced',\
                                                classes = np.unique(class_names),\
                                                y = ['F']*len(os.listdir(data_dir + '/train/fall_down'))+\
                                                    ['N']*len(os.listdir(data_dir +'/train/squat')))
class_weights=torch.tensor(class_weights,dtype=torch.float).cuda()
num_classes = len(class_weights)

train_transforms = transforms.Compose([transforms.Resize([input_size, input_size]),
                                       transforms.RandomHorizontalFlip(p = 0.9), # 經驗是影像擴增手法越多模型越容易overfiting
                                       transforms.ToTensor()])

#對測試集做變換
val_test_transforms = transforms.Compose([transforms.Resize([input_size, input_size]),
                                          transforms.ToTensor()])

dataset_tarin  = datasets.ImageFolder(data_dir + 'train', transform=train_transforms)
train_dataloader = torch.utils.data.DataLoader(dataset_tarin, batch_size, shuffle=True) # 影像集建議打亂，避免模型死記

dataset_val  = datasets.ImageFolder(data_dir + 'val', transform=val_test_transforms)
val_dataloader = torch.utils.data.DataLoader(dataset_val, batch_size, shuffle=False)

dataset_test = datasets.ImageFolder(data_dir + 'test', transform=val_test_transforms)
test_dataloader = torch.utils.data.DataLoader(dataset_test, 1, shuffle=False) # 測試集一定不打亂 # 指定測試資料的批大小為1

#%%
model_transfer = models.vgg19(pretrained=True) # 使用imageNet預訓練權重進行模型培訓

# print(model_transfer)

for i in model_transfer.features.named_parameters():
    layer_no = ''.join(c for c in i[0] if c.isdigit())
    if int(layer_no) <= 22: # 凍結的層數
        i[1].requires_grad = False # False凍結, True解凍
        
# for i in model_transfer.features.named_parameters():
#     print(i[0],i[1].requires_grad)
#%%
avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))

classifier = nn.Sequential(OrderedDict([
                          ('0', nn.Linear(512, 128)),
                          ('1', nn.Linear(128, 2)), # 使用兩個權連接層將資料維度收斂成2類(參考網站的經驗)
                          ('2', nn.LogSoftmax(dim=1)) # 再透過Softmax將資料壓縮成0~1之間
                          ])) 
model_transfer.avgpool = avgpool # 接一個平均池化在VGG19後面
model_transfer.classifier = classifier # 在平均池化後面接權連接層
use_cuda = torch.cuda.is_available()

if use_cuda:
    model_transfer = model_transfer.cuda()
# print(model_transfer)

#%%
# Loss and optimizer
criterion = torch.nn.CrossEntropyLoss(class_weights) # 設定損失函數
optimizer = torch.optim.Adam(model_transfer.parameters(), lr=learning_rate)# 設定超參數

#%%
# import EarlyStopping
class EarlyStopping:
    """Early stops the training if validation loss doesn't improve after a given patience."""
    def __init__(self, patience=20, verbose=False, delta=0):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.Inf
        self.delta = delta

    def __call__(self, val_loss, model):

        score = -val_loss

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
        elif score < self.best_score + self.delta:
            self.counter += 1
            print(f'EarlyStopping counter: {self.counter} out of {self.patience}') # 輸出當下已經沒有改善的次數
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model):
        '''
        Saves model when validation loss decrease.
        验证损失减少时保存模型。
        '''
        if self.verbose:
            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...') # 當模型驗證損失有下降就儲存該組權重
        torch.save(model, save_dir_weights) # 保存最優模型(附檔名為pkl)
        self.val_loss_min = val_loss

#%%
def train_model(train_loader, valid_loader, model, batch_size, patience, n_epochs):
    
    # to track the training loss as the model trains
    train_losses = []
    # to track the validation loss as the model trains
    valid_losses = []
    # to track the average training loss per epoch as the model trains
    avg_train_losses = []
    # to track the average validation loss per epoch as the model trains
    avg_valid_losses = [] 
    
    # initialize the early_stopping object
    early_stopping = EarlyStopping(patience, verbose=True)
    
    for epoch in range(1, n_epochs + 1):

        ###################
        # train the model #
        ###################
        model.train() # prep model for training
        for batch, (data, target) in enumerate(train_loader, 1):
            # clear the gradients of all optimized variables
            optimizer.zero_grad()
            # forward pass: compute predicted outputs by passing inputs to the model
            data   = data.cuda()
            target = target.cuda()
            output = model(data)
            # calculate the loss
            loss = criterion(output, target)
            # backward pass: compute gradient of the loss with respect to model parameters
            loss.backward()
            # perform a single optimization step (parameter update)
            optimizer.step()
            # record training loss
            train_losses.append(loss.item())

        ######################    
        # validate the model #
        ######################
        model.eval() # prep model for evaluation
        for data, target in valid_loader:
            # forward pass: compute predicted outputs by passing inputs to the model
            data   = data.cuda()
            target = target.cuda()
            output = model(data)
            # calculate the loss
            loss = criterion(output, target)
            # record validation loss
            valid_losses.append(loss.item())

        # print training/validation statistics 
        # calculate average loss over an epoch
        train_loss = np.average(train_losses)
        valid_loss = np.average(valid_losses)
        avg_train_losses.append(train_loss)
        avg_valid_losses.append(valid_loss)
        
        epoch_len = len(str(n_epochs))
        
        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +
                     f'train_loss: {train_loss:.5f} ' +
                     f'valid_loss: {valid_loss:.5f}')
        
        print(print_msg)
        
        # clear lists to track next epoch
        train_losses = []
        valid_losses = []
        
        # early_stopping needs the validation loss to check if it has decresed, 
        # and if it has, it will make a checkpoint of the current model
        early_stopping(valid_loss, model)
        
        if early_stopping.early_stop:
            print("Early stopping")
            break

    return  model, avg_train_losses, avg_valid_losses

#%%
model, train_loss, valid_loss = train_model(train_dataloader, val_dataloader, model_transfer, batch_size, patience, n_epochs)

#%%
# visualize the loss as the network trained
fig = plt.figure(figsize=(10,8))
plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')
plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')

# find position of lowest validation loss
minposs = valid_loss.index(min(valid_loss))+1 
plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')

plt.xlabel('epochs')
plt.ylabel('loss')
plt.ylim(0, 0.2) # consistent scale
plt.xlim(0, len(train_loss)+1) # consistent scale
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()
fig.savefig(save_dir_plot + '/vgg_loss_plot.png', bbox_inches='tight')

#%%
# initialize lists to monitor test loss and accuracy
class_correct = list(0. for i in range(len(test_dataloader.dataset)))
class_total = list(0. for i in range(len(test_dataloader.dataset)))

model.eval() # prep model for evaluation

for data, target in test_dataloader:
    if len(target.data) != 1:
        break
    # forward pass: compute predicted outputs by passing inputs to the model
    data   = data.cuda()
    target = target.cuda()
    output = model(data)
    # convert output probabilities to predicted class
    _, pred = torch.max(output, 1)
    # compare predictions to true label
    correct = np.squeeze(pred.eq(target.data.view_as(pred)))
    # calculate test accuracy for each object class

    label = target.data
    class_correct[label] += correct.item()
    class_total[label] += 1

# =============================================================================
# 計算正確率
# =============================================================================
for i in range(len(test_dataloader.dataset)):
    if class_total[i] > 0:
        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (
            str(i), 100 * class_correct[i] / class_total[i],
            np.sum(class_correct[i]), np.sum(class_total[i])))

print('\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (
    100. * np.sum(class_correct) / np.sum(class_total),
    np.sum(class_correct), np.sum(class_total)))
